{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import cProfile\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем крестики-нолики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN, clone=None):\n",
    "        if clone is not None:\n",
    "            self.n_rows, self.n_cols, self.n_win = clone.n_rows, clone.n_cols, clone.n_win\n",
    "            self.board = copy.deepcopy(clone.board)\n",
    "            self.curTurn = clone.curTurn\n",
    "            self.emptySpaces = None\n",
    "            self.boardHash = None\n",
    "        else:\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.n_win = n_win\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_game(env, pi1, pi2, random_crosses=False, random_naughts=True):\n",
    "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s, actions = env.getHash(), env.getEmptySpaces()\n",
    "        env.printBoard()\n",
    "        if env.curTurn == 1:\n",
    "            a = pi1.get_action(env, s, False)\n",
    "        else:\n",
    "            a = pi2.get_action(env, s, False)\n",
    "        observation, reward, done, info = env.step(a)\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "            env.printBoard()\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "            env.printBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_games(env, pi1, pi2, games):\n",
    "    results = {'x': 0, '0': 0, 'total': 0}\n",
    "    for i in range(games):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            s, actions = env.getHash(), env.getEmptySpaces()\n",
    "            if env.curTurn == 1:\n",
    "                a = pi1.get_action(env, s, False)\n",
    "            else:\n",
    "                a = pi2.get_action(env, s, False)\n",
    "            observation, reward, done, info = env.step(a)\n",
    "            if reward == 1:\n",
    "                results['x'] += 1\n",
    "            if reward == -1:\n",
    "                results['0'] += 1\n",
    "        results['total'] += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_user_game(env, pi1, pi2, random_crosses=False, random_naughts=True):\n",
    "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s, actions = env.getHash(), env.getEmptySpaces()\n",
    "        env.printBoard()\n",
    "        if env.curTurn == 1:\n",
    "            a = pi1.get_action(env, s, False)\n",
    "        else:\n",
    "            x = int(input())\n",
    "            y = int(input())\n",
    "            a = np.array([x, y])\n",
    "        observation, reward, done, info = env.step(a)\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "            env.printBoard()\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "            env.printBoard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть первая: крестики-нолики при помощи Q-обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL():\n",
    "    def __init__(self, desk_size):\n",
    "        self.q = dict()\n",
    "        self.desk_size = desk_size\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        self.old_hash = None\n",
    "        self.old_action = None\n",
    "        self.old_reward = None\n",
    "        \n",
    "    def forget_game(self):\n",
    "        self.old_hash = None\n",
    "        self.old_action = None\n",
    "        self.old_reward = None\n",
    "        \n",
    "    def get_action(self, env, state, rand):\n",
    "        if rand:\n",
    "            actions = env.getEmptySpaces()\n",
    "            idx = np.random.randint(len(actions))\n",
    "            action = actions[idx]\n",
    "        else:\n",
    "            s_hash = env.getHash()\n",
    "            if s_hash not in self.q:\n",
    "                self.q[s_hash] = np.zeros(self.desk_size**2)                \n",
    "            max_value = self.q[s_hash].max()\n",
    "            idxs = np.argwhere(self.q[s_hash]==max_value)\n",
    "            idx = np.random.randint(len(idxs))\n",
    "            action = env.action_from_int(idxs[idx])\n",
    "        return action\n",
    "    \n",
    "    # Обновляет политику после хода соперника\n",
    "    def update(self, old_hash, action, reward):\n",
    "        if self.old_hash is None:\n",
    "            self.old_hash = old_hash\n",
    "            self.old_action = action\n",
    "            self.old_reward = reward\n",
    "            return\n",
    "        \n",
    "        new_hash = old_hash\n",
    "        old_hash = self.old_hash\n",
    "        if old_hash not in self.q:\n",
    "            self.q[old_hash] = np.zeros(desk_size**2)\n",
    "        if new_hash not in self.q:\n",
    "            self.q[new_hash] = np.zeros(desk_size**2)\n",
    "            \n",
    "        old_act_int = env.int_from_action(self.old_action)\n",
    "        new_max_q = self.q[new_hash].max()\n",
    "        self.q[old_hash][old_act_int] = self.old_reward + self.gamma * new_max_q\n",
    "            \n",
    "        self.old_hash = new_hash\n",
    "        self.old_action = action\n",
    "        self.old_reward = reward\n",
    "    \n",
    "    # Обновляет политику сразу, так как игра закончилась\n",
    "    def change_q(self, reward, s_hash, action):\n",
    "        if s_hash not in self.q:\n",
    "            self.q[s_hash] = np.zeros(desk_size**2)\n",
    "        act_int = env.int_from_action(action)\n",
    "        self.q[s_hash][act_int] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_train(env, eps, n_steps, pi1, pi2):\n",
    "    env.reset()\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        pi = pi1 if env.curTurn == 1 else pi2\n",
    "        rand = (random.random() < eps)\n",
    "        old_hash = env.getHash()\n",
    "        action = pi.get_action(env, old_hash, rand)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_hash = next_state[0]\n",
    "        if abs(reward) == 1:\n",
    "            reward = abs(reward)\n",
    "            another_pi = pi1 if env.curTurn == 1 else pi2\n",
    "            another_pi.change_q(-reward, another_pi.old_hash, another_pi.old_action)\n",
    "            pi.change_q(reward, old_hash, action)\n",
    "        if reward == -10:\n",
    "            pi.change_q(reward, old_hash, action)\n",
    "        pi.update(old_hash, action, reward)\n",
    "        if done:\n",
    "            env.reset()\n",
    "            old_hash = env.getHash()\n",
    "        else:\n",
    "            old_hash = next_hash\n",
    "        if done:\n",
    "            pi1.forget_game()\n",
    "            pi2.forget_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [02:16<00:00, 7315.40it/s]\n"
     ]
    }
   ],
   "source": [
    "desk_size = 3\n",
    "env = TicTacToe(n_rows=desk_size, n_cols=desk_size, n_win=3)\n",
    "pi1 = QL(desk_size)\n",
    "pi2 = QL(desk_size)\n",
    "\n",
    "q_train(env, 0.1, 1000000, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "(2, 0)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "1\n",
      "1\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "(1, 0)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "0\n",
      "0\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "(2, 2)\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "0\n",
      "1\n",
      "-------------\n",
      "| o | o |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "(2, 1)\n",
      "Крестики выиграли!\n",
      "-------------\n",
      "| o | o |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| x | x | x | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# покажем, что агент умеет выигрывать\n",
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "plot_user_game(env, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Побед крестиков: 0.7000000000000001%\n",
      "Побед ноликов: 0.0%\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "results = test_games(env, pi1, pi2, 1000)\n",
    "print('Побед крестиков: {}%'.format(results['x'] / results['total'] * 100))\n",
    "print('Побед ноликов: {}%'.format(results['0'] / results['total'] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000000/20000000 [45:31<00:00, 7321.30it/s] \n"
     ]
    }
   ],
   "source": [
    "desk_size = 4\n",
    "env = TicTacToe(n_rows=desk_size, n_cols=desk_size, n_win=4)\n",
    "pi1 = QL(desk_size)\n",
    "pi2 = QL(desk_size)\n",
    "\n",
    "q_train(env, 0.1, 20000000, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "(2, 1)\n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "1\n",
      "1\n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "|   | o |   |   | \n",
      "-----------------\n",
      "|   | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "(0, 3)\n",
      "-----------------\n",
      "|   |   |   | x | \n",
      "-----------------\n",
      "|   | o |   |   | \n",
      "-----------------\n",
      "|   | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "0\n",
      "2\n",
      "-----------------\n",
      "|   |   | o | x | \n",
      "-----------------\n",
      "|   | o |   |   | \n",
      "-----------------\n",
      "|   | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "(1, 2)\n",
      "-----------------\n",
      "|   |   | o | x | \n",
      "-----------------\n",
      "|   | o | x |   | \n",
      "-----------------\n",
      "|   | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "2\n",
      "0\n",
      "-----------------\n",
      "|   |   | o | x | \n",
      "-----------------\n",
      "|   | o | x |   | \n",
      "-----------------\n",
      "| o | x |   |   | \n",
      "-----------------\n",
      "|   |   |   |   | \n",
      "-----------------\n",
      "(3, 0)\n",
      "Крестики выиграли!\n",
      "-----------------\n",
      "|   |   | o | x | \n",
      "-----------------\n",
      "|   | o | x |   | \n",
      "-----------------\n",
      "| o | x |   |   | \n",
      "-----------------\n",
      "| x |   |   |   | \n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=4, n_cols=4, n_win=4)\n",
    "plot_user_game(env, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Побед крестиков: 1.7000000000000002%\n",
      "Побед ноликов: 1.2%\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=4, n_cols=4, n_win=4)\n",
    "results = test_games(env, pi1, pi2, 1000)\n",
    "print('Побед крестиков: {}%'.format(results['x'] / results['total'] * 100))\n",
    "print('Побед ноликов: {}%'.format(results['0'] / results['total'] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть вторая: добавим нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "STEPS_PER_UPDATE = 4\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.002\n",
    "REPLAY_LEN = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.hiddim = 128\n",
    "        self.conv_size = 3\n",
    "        self.conv_number = 32\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_input = (state_dim - self.conv_size + 1)**2 * self.conv_number\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=self.conv_number, kernel_size=self.conv_size)\n",
    "        self.liner1 = nn.Linear(self.lin_input, self.hiddim)\n",
    "        self.liner2 = nn.Linear(self.hiddim, action_dim)\n",
    "    def forward(self, x):\n",
    "        result = torch.flatten(self.conv(x), start_dim=1)\n",
    "        result = self.relu(self.liner1(result))\n",
    "        result = self.liner2(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_duo_model(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN_duo_model, self).__init__()\n",
    "        self.hiddim = 32\n",
    "        self.conv_size = 3\n",
    "        self.conv_number = 32\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_input = (state_dim - self.conv_size + 1)**2 * self.conv_number\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=self.conv_number, kernel_size=self.conv_size)\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(self.lin_input, self.hiddim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hiddim, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(self.lin_input, self.hiddim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hiddim, action_dim)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = torch.flatten(self.conv(x), start_dim=1)\n",
    "        values = self.value(features)\n",
    "        advantages = self.advantage(features)\n",
    "        result = values + advantages - torch.mean(advantages, dim=1, keepdim=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.hiddim = 32\n",
    "        self.conv_size = 3\n",
    "        self.conv_number = 32\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_input = (state_dim - self.conv_size + 1)**2 * self.conv_number\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=self.conv_number, kernel_size=self.conv_size)\n",
    "        self.liner1 = nn.Linear(self.lin_input, self.hiddim)\n",
    "        self.liner2 = nn.Linear(self.hiddim, action_dim)\n",
    "    def forward(self, x):\n",
    "        result = torch.flatten(self.conv(x), start_dim=1)\n",
    "        result = self.relu(self.liner1(result))\n",
    "        result = self.liner2(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, is_duo):\n",
    "        self.steps = 0\n",
    "        self.state_dim = state_dim\n",
    "        self.model = DQN_duo_model(state_dim, action_dim) if is_duo else DQN_model(state_dim, action_dim)\n",
    "        self.model_t = copy.deepcopy(self.model)\n",
    "        self.replay = deque(maxlen=REPLAY_LEN)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        self.old_hash = None\n",
    "        self.old_action = None\n",
    "        self.old_reward = None\n",
    "        self.old_done = None\n",
    "        \n",
    "    def forget_game(self):\n",
    "        self.old_hash = None\n",
    "        self.old_action = None\n",
    "        self.old_reward = None\n",
    "        self.old_done = None\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay, self.batch_size)\n",
    "        return list(zip(*batch))\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        state, action, next_state, reward, done = batch\n",
    "        state = torch.tensor(np.array(state, dtype=np.float32).reshape(BATCH_SIZE, 1,self.state_dim, -1))\n",
    "        action = torch.tensor(np.array(action, dtype=np.int))\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.float32).reshape(BATCH_SIZE, 1,self.state_dim, -1))\n",
    "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
    "        done = torch.tensor(np.array(done, dtype=np.float32))\n",
    "        \n",
    "        Qt = self.model_t(next_state).max(dim=1)[0]\n",
    "        Qt[done == 1] = 0\n",
    "        Qt = reward + torch.mul(Qt, GAMMA)\n",
    "        \n",
    "        Q_all = self.model(state)\n",
    "        Q = Q_all[np.arange(Q_all.shape[0]), action]\n",
    "        \n",
    "        self.optim.zero_grad() \n",
    "        loss = self.criterion(Q, Qt)\n",
    "        loss.backward()\n",
    "        self.optim.step()           \n",
    "\n",
    "    def get_action(self, env, state, rand):\n",
    "        if rand:\n",
    "            actions = env.getEmptySpaces()\n",
    "            idx = np.random.randint(len(actions))\n",
    "            action = actions[idx]\n",
    "        else:\n",
    "            if type(state) == str:\n",
    "                state = [int(x) for x in state]\n",
    "            with torch.no_grad():\n",
    "                state = np.array(state, dtype=np.float32).reshape(1, 1, self.state_dim, -1)\n",
    "                state = torch.tensor(state)\n",
    "                actions = self.model(state).numpy()\n",
    "                action = np.argmax(actions)\n",
    "                action = env.action_from_int(action)\n",
    "        return action\n",
    "    \n",
    "    # Обновляет политику после хода соперника\n",
    "    def update(self, reward, old_hash, action, done):\n",
    "        if self.old_hash is None:\n",
    "            self.old_hash = old_hash\n",
    "            self.old_action = action\n",
    "            self.old_reward = reward\n",
    "            self.old_done = done\n",
    "            return\n",
    "        \n",
    "        new_hash = old_hash\n",
    "        old_hash = self.old_hash\n",
    "        old_act_int = env.int_from_action(self.old_action)\n",
    "        \n",
    "        self.replay.append((old_hash, old_act_int, new_hash, self.old_reward, self.old_done))\n",
    "        self.old_hash = new_hash\n",
    "        self.old_action = action\n",
    "        self.old_reward = reward\n",
    "        self.old_done = done\n",
    "        \n",
    "        if (self.steps % STEPS_PER_UPDATE == 0) and (len(self.replay) > BATCH_SIZE):\n",
    "            batch = self.sample_batch()\n",
    "            self.train_step(batch)\n",
    "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
    "            self.model_t = copy.deepcopy(self.model)\n",
    "        self.steps += 1\n",
    "    \n",
    "    # Обновляет политику сразу, так как игра закончилась    \n",
    "    def quick_update(self, reward, state, action, done, next_state):\n",
    "        act_int = env.int_from_action(action)\n",
    "        self.replay.append((state, act_int, next_state, reward, done))\n",
    "        if (self.steps % STEPS_PER_UPDATE == 0) and (len(self.replay) > BATCH_SIZE):\n",
    "            batch = self.sample_batch()\n",
    "            self.train_step(batch)\n",
    "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
    "            self.model_t = copy.deepcopy(self.model)\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_train(env, eps, desk_size, n_steps, pi1, pi2):\n",
    "    dummy_state = [0] * desk_size**2\n",
    "    env.reset()\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        pi = pi1 if env.curTurn == 1 else pi2\n",
    "        rand = (random.random() < eps)\n",
    "        old_hash = env.getHash()\n",
    "        old_hash = [int(x) for x in old_hash]\n",
    "        action = pi.get_action(env, old_hash, rand)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_hash = next_state[0]\n",
    "        next_hash = [int(x) for x in next_hash]\n",
    "        if abs(reward) == 1:\n",
    "            reward = abs(reward)\n",
    "            another_pi = pi1 if env.curTurn == 1 else pi2\n",
    "            another_pi.quick_update(-reward, another_pi.old_hash, another_pi.old_action, done, dummy_state)\n",
    "            pi.quick_update(reward, old_hash, action, done, dummy_state)\n",
    "        if reward == -10:\n",
    "            pi.quick_update(reward, old_hash, action, done, dummy_state)\n",
    "        pi.update(reward, old_hash, action, done)\n",
    "        if done:\n",
    "            env.reset()\n",
    "            old_hash = env.getHash()\n",
    "            old_hash = [int(x) for x in old_hash]\n",
    "            pi1.forget_game()\n",
    "            pi2.forget_game()\n",
    "        else:\n",
    "            old_hash = next_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [09:22<00:00, 710.98it/s]\n"
     ]
    }
   ],
   "source": [
    "desk_size = 3\n",
    "eps = 0.2\n",
    "n_steps = 400000\n",
    "env = TicTacToe(n_rows=desk_size, n_cols=desk_size, n_win=3)\n",
    "pi1 = DQN(state_dim=desk_size, action_dim=desk_size**2, is_duo=False)\n",
    "pi2 = DQN(state_dim=desk_size, action_dim=desk_size**2, is_duo=False)\n",
    "\n",
    "dqn_train(env, eps, desk_size, n_steps, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "1\n",
      "1\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "0\n",
      "2\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "0\n",
      "0\n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "Крестики выиграли!\n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x | x | x | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "plot_user_game(env, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Побед крестиков: 0.0%\n",
      "Побед ноликов: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Так как политика детерминированная, исход всегла одинаков\n",
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "results = test_games(env, pi1, pi2, 1000)\n",
    "print('Побед крестиков: {}%'.format(results['x'] / results['total'] * 100))\n",
    "print('Побед ноликов: {}%'.format(results['0'] / results['total'] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [06:24<00:00, 519.77it/s]\n"
     ]
    }
   ],
   "source": [
    "desk_size = 3\n",
    "eps = 0.2\n",
    "n_steps = 200000\n",
    "env = TicTacToe(n_rows=desk_size, n_cols=desk_size, n_win=3)\n",
    "pi1 = DQN(state_dim=desk_size, action_dim=desk_size**2, is_duo=True)\n",
    "pi2 = DQN(state_dim=desk_size, action_dim=desk_size**2, is_duo=True)\n",
    "\n",
    "dqn_train(env, eps, desk_size, n_steps, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "0\n",
      "0\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "0\n",
      "2\n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "2\n",
      "2\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "|   | x | x | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "1\n",
      "0\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "Крестики выиграли!\n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "plot_user_game(env, pi1, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Побед крестиков: 100.0%\n",
      "Побед ноликов: 0.0%\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "results = test_games(env, pi1, pi2, 1000)\n",
    "print('Побед крестиков: {}%'.format(results['x'] / results['total'] * 100))\n",
    "print('Побед ноликов: {}%'.format(results['0'] / results['total'] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
